{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import datetime\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torchaudio.transforms import AmplitudeToDB, MelSpectrogram, Resample\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"../../ml/urbansound8k\")\n",
    "SOURCE_DIR = BASE_DIR\n",
    "DEST_DIR = BASE_DIR/\"processed\"\n",
    "INDEX_PATH = BASE_DIR/\"UrbanSound8K.csv\"\n",
    "RATE = 22050\n",
    "N_FFT = 256\n",
    "HOP_LENGTH = N_FFT // 2\n",
    "N_MELS = 100\n",
    "SPEC_TIMESTEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob(\"*.wav\", root_dir=SOURCE_DIR)\n",
    "# filepaths = [SOURCE_DIR/file for file in files]\n",
    "index = {}\n",
    "with open(INDEX_PATH, encoding='UTF-8') as index_file:\n",
    "    csv_reader = csv.DictReader(index_file)\n",
    "    index = [row for row in csv_reader]\n",
    "print(index[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filepath):\n",
    "    audio, sr = torchaudio.load(filepath)\n",
    "\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "\n",
    "    if sr != RATE:\n",
    "        resampler = Resample(sr, RATE)\n",
    "        audio = resampler(audio)\n",
    "\n",
    "    num_samples = audio.shape[-1]\n",
    "    total_duration = num_samples / RATE\n",
    "\n",
    "    return audio, RATE, num_samples, total_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mel_spectrogram(audio):\n",
    "    spec_transformer = MelSpectrogram(RATE, N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "    mel_spec = spec_transformer(audio).squeeze(0)\n",
    "\n",
    "    amplitude_to_db_transformer = AmplitudeToDB()\n",
    "    mel_spec_db = amplitude_to_db_transformer(mel_spec)\n",
    "\n",
    "    return mel_spec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_timings(spec):\n",
    "    num_frames = spec.shape[-1]\n",
    "    time_per_frame = HOP_LENGTH / RATE\n",
    "    time_values = (torch.arange(0, num_frames) * time_per_frame).numpy()\n",
    "    return num_frames, time_per_frame, time_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_folds():\n",
    "    for record in tqdm(index, total=len(index)):\n",
    "        fold_dir = Path(f\"fold{record['fold']}\")\n",
    "        file_name = record['slice_file_name']\n",
    "        source = SOURCE_DIR/fold_dir/file_name\n",
    "        dest_dir = DEST_DIR/fold_dir\n",
    "        Path.mkdir(dest_dir, exist_ok=True, parents=True)\n",
    "        dest_file = dest_dir/f\"{file_name}.spec\"\n",
    "\n",
    "        audio, sr = preprocess(source)\n",
    "        mel_spec_db = make_mel_spectrogram(audio)\n",
    "\n",
    "        torch.save(mel_spec_db, dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_saved_spec(path):\n",
    "    spec = torch.load(path)\n",
    "    plot_spec(spec)\n",
    "\n",
    "def plot_spec(spec):\n",
    "    _, _, time_values = frame_timings(spec)\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(4,4))\n",
    "\n",
    "    axs.set_xticks(np.arange(0, len(time_values), step=int(len(time_values)/5)), np.round(time_values[::int(len(time_values)/5)], 2))\n",
    "\n",
    "    axs.imshow(spec.numpy(), origin='lower')\n",
    "    plt.show()\n",
    "\n",
    "plot_saved_spec(\"/home/davery/ml/urbansound8k/processed/fold1/203356-3-0-3.wav-0.spec\")\n",
    "Audio(\"/home/davery/ml/urbansound8k/fold1/203356-3-0-0.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_audio(audio):\n",
    "\n",
    "    mel_spec_db = make_mel_spectrogram(audio)\n",
    "\n",
    "    _, _, time_values = frame_timings(mel_spec_db)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8,4))\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "    axs[0].set_xlabel('Time')\n",
    "    axs[0].set_ylabel('Amplitude')\n",
    "    axs[0].plot(audio.t().numpy())\n",
    "\n",
    "    axs[1].set_xticks(np.arange(0, len(time_values), step=int(len(time_values)/5)), np.round(time_values[::int(len(time_values)/5)], 2))\n",
    "    axs[1].imshow(mel_spec_db.numpy())\n",
    "    plt.show()\n",
    "    Audio(audio, rate=RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spectrogram(spec: torch.Tensor, chunk_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Splits a spectrogram tensor into equal-sized chunks along the time axis.\n",
    "\n",
    "    This function divides a 2D spectrogram tensor into smaller chunks of a specified size. If the spectrogram\n",
    "    cannot be evenly divided, the remaining part is zero-padded at the end to form a complete chunk. The output\n",
    "    is a 3D tensor where the first dimension corresponds to the chunk index.\n",
    "\n",
    "    Parameters:\n",
    "    spec (torch.Tensor): A 2D tensor representing the spectrogram with shape (frequency_bins, time_steps).\n",
    "    chunk_size (int): The desired number of time steps in each chunk.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A 3D tensor with shape (num_chunks, frequency_bins, chunk_size), where num_chunks is the\n",
    "                  number of total chunks calculated based on the spectrogram size and chunk_size.\n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate number of chunks needed without padding\n",
    "    new_spec = spec.clone()\n",
    "    num_chunks = new_spec.shape[1] // chunk_size\n",
    "\n",
    "    # calculate the size of the remainder\n",
    "    remainder = new_spec.shape[1] % chunk_size\n",
    "    if remainder != 0:\n",
    "        # if there is a remainder, we need to pad the spec\n",
    "        padding_size = chunk_size - remainder\n",
    "        padding = torch.zeros(\n",
    "            (new_spec.shape[0], padding_size),\n",
    "            dtype=new_spec.dtype,\n",
    "            device=new_spec.device,\n",
    "        )\n",
    "        new_spec = torch.cat([new_spec, padding], dim=1)\n",
    "        num_chunks += 1\n",
    "    # Use unfold to split the tensor along the time axis\n",
    "    unfolded = new_spec.unfold(dimension=1, size=chunk_size, step=chunk_size)\n",
    "\n",
    "    # unfolded has shape (frequency_bins, num_chunks, chunk_size)\n",
    "    # We need to transpose it to get (num_chunks, frequency_bins, chunk_size)\n",
    "    chunks = unfolded.transpose(0, 1)\n",
    "    return chunks.contiguous()\n",
    "\n",
    "audio, sr, _, _ = preprocess(\"/home/davery/ml/urbansound8k/fold1/203356-3-0-3.wav\")\n",
    "mel_spec_db = make_mel_spectrogram(audio)\n",
    "print(mel_spec_db.shape)\n",
    "split_spec = split_spectrogram(mel_spec_db, SPEC_TIMESTEPS)\n",
    "print(split_spec.shape)\n",
    "\n",
    "# minispec = torch.tensor([[5,6,7,8,9],[5,6,7,8,9],[5,6,7,8,9],[5,6,7,8,9],[5,6,7,8,9]])\n",
    "# print(f\"{minispec=}\")\n",
    "# print(f\"{minispec.shape=}\")\n",
    "# split = split_spectrogram(minispec, 2)\n",
    "# print(f\"{split=}\")\n",
    "# print(f\"{split.shape=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spec(mel_spec_db)\n",
    "for i in range(len(split_spec)):\n",
    "    plot_spec(split_spec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_split_specs():\n",
    "    shutil.rmtree(DEST_DIR)\n",
    "    count = 0\n",
    "    for record in tqdm(index, total=len(index)):\n",
    "        fold_dir_name = f\"fold{record['fold']}\"\n",
    "        file_name = record['slice_file_name']\n",
    "        source = SOURCE_DIR/fold_dir_name/file_name\n",
    "        fold_dir = DEST_DIR/fold_dir_name\n",
    "        Path.mkdir(fold_dir, exist_ok=True, parents=True)\n",
    "\n",
    "        audio, sr, num_samples, total_duration = preprocess(source)\n",
    "        mel_spec_db = make_mel_spectrogram(audio)\n",
    "        chunks = split_spectrogram(mel_spec_db, SPEC_TIMESTEPS)\n",
    "\n",
    "        for i in range(len(chunks)):\n",
    "            dest_file = fold_dir/f\"{file_name}-{i}.spec\"\n",
    "            torch.save(chunks[i], dest_file)\n",
    "            count += 1\n",
    "    print(f\"{count} chunk specs saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_split_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, spec_dir, transform=None, target_transform=None):\n",
    "        self.spec_dir = spec_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.spec_paths = [os.path.join(self.spec_dir, file) for file in glob(\"*.spec\", root_dir=self.spec_dir)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spec_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.spec_paths[idx]\n",
    "        file_name = os.path.basename(file_path)\n",
    "        spec = torch.load(self.spec_paths[idx])\n",
    "        parts = file_name.split('-')\n",
    "        label = int(parts[1])\n",
    "        return spec, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 100, 1000)  # Batch size of 1, 1 channel, 100x100 image\n",
    "            dummy_output = self.pool1(self.conv1(dummy_input))\n",
    "            self.flat_features = int(torch.numel(dummy_output) / dummy_output.shape[0])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flat_features, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1,self.flat_features)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN_2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(BasicCNN_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1a = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 100, 500)  # Batch size of 1, 1 channel, 100x100 image\n",
    "            dummy_output = self.pool1(self.conv1(dummy_input))\n",
    "            self.flat_features = int(torch.numel(dummy_output) / dummy_output.shape[0])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flat_features, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1a(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1,self.flat_features)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN_3(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(BasicCNN_3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv1a = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv2a = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv3a = nn.Conv2d(256, 256, kernel_size=5, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 100, 1000)  # Batch size of 1, 1 channel, 100x100 image\n",
    "            dummy_output = self.pre_flatten(dummy_input)\n",
    "            self.flat_features = int(torch.numel(dummy_output) / dummy_output.shape[0])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flat_features, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def pre_flatten(self, x):\n",
    "        x = self.relu(self.conv1a(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv2a(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.conv3a(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_flatten(x)\n",
    "        x = x.view(-1,self.flat_features)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data = data.to(\"cuda\")\n",
    "        # print(data.shape)\n",
    "        data = data.unsqueeze(1)\n",
    "        data = F.normalize(data, dim=2)\n",
    "\n",
    "        target = target.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        epoch_total += target.size(0)\n",
    "        epoch_correct += (predicted == target).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    avg_acc = 100.0 * epoch_correct / epoch_total\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_dataset = SpectrogramDataset(spec_dir=DEST_DIR/\"fold1\")\n",
    "all_folds = [SpectrogramDataset(spec_dir=DEST_DIR/f\"fold{i}\") for i in range(1,11)]\n",
    "spectrogram_datasets = ConcatDataset(all_folds)\n",
    "spectrogram_dataloader = DataLoader(spectrogram_dataset, batch_size=32, num_workers=8, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicCNN_3()\n",
    "model = model.to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = loss_function.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param_group in optimizer.param_groups:\n",
    "    # param_group['lr'] = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, spectrogram_dataloader, optimizer, loss_function)\n",
    "    print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Epoch {epoch+1}/{num_epochs}, \", end='')\n",
    "    print(f\"Train Loss: {train_loss:.5f}, Train Accuracy: {train_acc:.2f}%\", end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbansound8k",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
